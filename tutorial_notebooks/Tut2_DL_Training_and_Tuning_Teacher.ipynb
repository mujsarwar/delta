{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 2 - Training and Tuning of Deep Learning Models in Python\n",
    "\n",
    "The lecture has introduced you to foundations of neural networks. Therefore, in today's tutorial notebook we will revisit the forward pass performed in shallow and deep neural networks on tabular data. This will help us understand the functionality behind **sklearn's MLPRegressor**, as well as which hyperparameters of neural networks can be tuned to improve their performance. \n",
    "\n",
    "Here is the outline of the demo notebook:\n",
    "*   Forward pass of shallow and deep neural networks in numpy (Demo). \n",
    "*   Implementation of two MLPRegressor models with varying depth using sklearn (Excercise 1).\n",
    "*   Tuning of MLPRegressor using Optuna (Excercise 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Forward Pass of shallow and deep Neural Networks with Numpy** (DEMO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.1. Forward Pass in Neural Networks with a single hidden Layer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's revisit the forward pass in a shallow neural network, using the below illustration: <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Shallow Neural Network](https://github.com/Humboldt-WI/demopy/raw/main/Shallow_Neural_Network_Forward_Pass.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a neural network with **<em>m</em>** input variables, which are fed to a single hidden layer with **<em>w</em>** = 2 units. The hidden layer makes use of the activation function **<em>g</em>** to transform the latent representation of the inputs nonlinearly. The output of the nonlinear transformation is then fed to the final layer, i.e., the prediction layer, which produces the estimates of the target variable **<em>y</em>**.<br>\n",
    "\n",
    "Now, imagine you have a batch of 20 samples with 5 predictor features and a single continuous target variable. You are tasked to implement the forward pass of the above-described neural network with numpy. First, we will simulate our batch of data by drawing samples from a standard normal distribution using numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of batch of simulated data:  (20, 5)\n"
     ]
    }
   ],
   "source": [
    "#Simulation of a batch of 20 samples with 5 features:\n",
    "w=2\n",
    "m=5\n",
    "num_samples=20\n",
    "data_batch=np.random.normal(size=m*num_samples,loc=0.0,scale=1.0).reshape(num_samples,m)\n",
    "#We create an artificial target variable using the exponential function in numpy:\n",
    "target=np.exp(np.dot(data_batch,np.random.normal(size=m).reshape(-1,1)))\n",
    "print('Shape of batch of simulated data: ',data_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will initialize the trainable parameters of the hidden layer and the output layer of the shallow network. The shape of the weight matrices in the hidden layer  and the output layer are **<em>(m,w)</em>** and **<em>(w,1)</em>**. The shape of the bias weights is determined by the  number of units in each layer. Since one of the main advantages of using neural networks is their ability to approximate nonlinear relationships, we will define two nonlinear activation functions, i.e., ReLU and Tanh. We will use both to nonlinearly transform the latent feature space produced by the matrix multiplication of the network weights with the synthetic batch of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the trainable weight matrices of the shallow neural network:\n",
      "- W of hidden layer: (5, 2)\n",
      "- Bias of hidden layer: (2,) \n",
      "\n",
      "- W of output layer: (2, 1)\n",
      "- Bias of output layer: (1,) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "w_hidden_layer=np.random.normal(size=m*w).reshape(m,w)\n",
    "bias_hidden_layer=np.random.normal(size=w)\n",
    "\n",
    "w_output_layer=np.random.normal(size=w).reshape(-1,1)\n",
    "bias_output_layer=np.random.normal(size=1)\n",
    "\n",
    "print('Shape of the trainable weight matrices of the shallow neural network:')\n",
    "print('- W of hidden layer:',w_hidden_layer.shape)\n",
    "print('- Bias of hidden layer:',bias_hidden_layer.shape,'\\n')\n",
    "print('- W of output layer:',w_output_layer.shape)\n",
    "print('- Bias of output layer:',bias_output_layer.shape,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the linear latent representation of the input variables:  (20, 2)\n"
     ]
    }
   ],
   "source": [
    "#Linear latent feature space of inputs: generated with matrix multiplication,\n",
    "#we implement the latter with the function numpy.dot:\n",
    "linear_latent_representation=np.dot(data_batch,w_hidden_layer) + bias_hidden_layer\n",
    "print('Shape of the linear latent representation of the input variables: ',linear_latent_representation.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The matrix multiplication of the weights with the input variables produces a linear latent representation consisting of 2 units for each of the 20 synthetic data samples. The bias weights array has only 2 trainable weights. At first sight, the addition of one-dimensional array to a two-dimensional matrix with the shape (20,2) seems conceptually impossible in the context of linear algebra. However, due to broadcasting, numpy efficiently adds the bias weights to each row of the latent representation. For more details on broadcasting, we refer the reader to: https://numpy.org/doc/stable/user/basics.broadcasting.html. <br>\n",
    "\n",
    "Next, we create a well-documented function implementing ReLU and Tanh: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nonlinear_func(x:np.ndarray,\n",
    "                   activation_name:str)->np.ndarray:\n",
    "    \"\"\"Computes ReLU or Tanh transformations of the inputs.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    x: np.ndarray\n",
    "        The linearly transformed inputs.\n",
    "    activation_name: str\n",
    "        The name of the activation function, i.e., ReLU or Tanh.\n",
    "    \n",
    "    Returns:\n",
    "    ----------\n",
    "    nonlinear_x: np.ndarray\n",
    "        The nonlinearly transformed inputs.\n",
    "    \n",
    "    Raises:\n",
    "    ----------\n",
    "    ValueError: If specified activation_name is not in the list [ReLU, Tanh].\n",
    "    \"\"\"\n",
    "    \n",
    "    if activation_name not in ['ReLU','Tanh']:\n",
    "        raise ValueError(f'The currently specified activation function {activation_name} is not in the list of supported nonlinearities.')\n",
    "        \n",
    "    if activation_name=='Tanh':\n",
    "        nonlinear_x=np.tanh(x)\n",
    "    else:\n",
    "        nonlinear_x=x*(x>0.0)\n",
    "        \n",
    "    return nonlinear_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see in the above function, we specify the type of the input parameters and the type of the output values of our function. Also, we provide a brief summary of what the function does, what the input parameters and the return value represent. Proper documentation of your functions improves the readability of your code a lot especially for someone, who is examining your code for the first time. For more details on the annotation of python functions, we refer the reader to: https://peps.python.org/pep-3107/#fundamentals-of-function-annotations. <br>\n",
    "\n",
    "Next, we will pass the linearly weighted inputs through our nonlinear activations, and we will plot the distribution of the resulting values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "relu_latent=nonlinear_func(x=linear_latent_representation,activation_name='ReLU')\n",
    "tanh_latent=nonlinear_func(x=linear_latent_representation,activation_name='Tanh')\n",
    "\n",
    "fig,ax=plt.subplots(nrows=1,ncols=2,figsize=(10,3))\n",
    "ax[0].hist(relu_latent.flatten())\n",
    "ax[0].set_title('ReLU latent Feature Space')\n",
    "\n",
    "ax[1].hist(tanh_latent.flatten())\n",
    "ax[1].set_title('Tanh latent Feature Space')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above histogram plots highlight the difference in the distribution of the latent features. While ReLU produces a right-skewed representation of the inputs, Tanh generates a bimodal latent feature space. Generally, the ReLU activation is prefered over Tanh in neural networks, especially in deep neural models. However, Tanh could also be applied in rather shallow neural models. The main limitation of Tanh, and generally speaking of activations with a lower and an upper bound, is that output values very close to the saturation regions, i.e., -1 and 1 in case of Tanh, receive almost no gradients in the backward pass. Therefore, bounded activation functions suffer from the vanishing gradients problem.<br>\n",
    "\n",
    "Next, we produce the predictions of our synthetic target:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions shape:  (20, 1)\n"
     ]
    }
   ],
   "source": [
    "relu_predictions=np.dot(relu_latent,w_output_layer)+bias_output_layer\n",
    "tanh_predictions=np.dot(tanh_latent,w_output_layer)+bias_output_layer\n",
    "print('Predictions shape: ',relu_predictions.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the quality of the estimations, we define a function computing the mean-squared error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(true_target:np.ndarray,\n",
    "        predictions:np.ndarray)->np.float64:\n",
    "    '''\n",
    "    Computes the mean-squared error (MSE) between the true values of the target variable and the predictions of an ML model.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    true_target: np.ndarray\n",
    "        The true values.\n",
    "    predictions: np.ndarray\n",
    "        The estimation of the target values.\n",
    "        \n",
    "    Returns:\n",
    "    -----------\n",
    "    mse_score: np.float\n",
    "        The error between the true values and the predictions measured in terms of MSE.\n",
    "    '''\n",
    "    mse_score=np.mean((true_target-predictions)**2)\n",
    "    return mse_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE from ReLU-Network:  9345.002\n",
      "MSE from Tanh-Network:  9427.176\n"
     ]
    }
   ],
   "source": [
    "print('MSE from ReLU-Network: ',round(mse(true_target=target,predictions=relu_predictions),3))\n",
    "print('MSE from Tanh-Network: ',round(mse(true_target=target,predictions=tanh_predictions),3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have performed the forward pass of a neural network with a single hidden layer. Without any training of the two versions of the neural network the results in terms of MSE seem pretty close to each other, as both versions use the same randomly initialized weights in the hidden layer. We will use our mse function in Section 2 and Section 3, where we will compare the performance of two trained neural networks and we will implement neural network tuning, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.1. Forward Pass in deep Neural Networks with several hidden Layers**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we move on to more coding, let's recap the forward pass in deep neural networks: <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Deep Neural Network](https://github.com/Humboldt-WI/demopy/raw/main/Deep_Neural_Networks_Forward_Pass.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above visualization shows that in a deep neural networks the layers are structured in a chain, where the output of each layer is fed as an input to every following layer. By \"fed\", we mean that we compute the matrix multiplication between the latent representation of the previous layer and the trainable weights of the current layer, add the biases and apply a nonlinear activation function. Therefore, the output shape of each hidden layer determines the shape of the weight matrices of every following hidden layer.<br>\n",
    "\n",
    "Below, we create a function, which computes the forward pass of an arbitrarily ReLU-based deep neural network implemented in numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_neural_net(hidden_units:list,\n",
    "                    inputs:np.ndarray,\n",
    "                    print_layer_shape:bool)->np.ndarray:\n",
    "    '''\n",
    "    Computes the forward pass with an arbitrarily deep feedforward neural network.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    hidden_units: list\n",
    "        A list of hidden units, which also determines the number of hidden layers in the network.\n",
    "    inputs: np.ndarray\n",
    "        A two-dimensional numpy matrix containing the data samples in the rows, and the predictor variables in the columns.\n",
    "    print_layer_shape: bool\n",
    "        Determines whether to print the shape of the latent representation produced by each hidden layer and the final prediction layer.\n",
    "\n",
    "    Returns:\n",
    "    -----------\n",
    "    predictions: np.ndarray\n",
    "        The estimation of the target.\n",
    "    '''\n",
    "    \n",
    "    current_linear_representation=inputs.copy()\n",
    "    if print_layer_shape==True:\n",
    "        print('Inputs shape: ',inputs.shape,'\\n')\n",
    "    \n",
    "\n",
    "    for nr_units_idx in range(0,len(hidden_units)):\n",
    "        if nr_units_idx==0:\n",
    "            current_weights=np.random.normal(size=inputs.shape[1]*hidden_units[nr_units_idx]).reshape(inputs.shape[1],hidden_units[nr_units_idx])\n",
    "        else:\n",
    "            current_weights=np.random.normal(size=previous_matrix_shape[1]*hidden_units[nr_units_idx]).reshape(previous_matrix_shape[1],hidden_units[nr_units_idx])\n",
    "        current_bias_weights=np.random.normal(size=hidden_units[nr_units_idx])\n",
    "\n",
    "        #At each hidden layer we overwrite the linear representation passed to the nonlinearity: \n",
    "        current_linear_representation=np.dot(current_linear_representation,current_weights)+current_bias_weights\n",
    "\n",
    "        #ReLU-based transformation:\n",
    "        current_nonlinear_representation=nonlinear_func(x=current_linear_representation,\n",
    "                                                        activation_name='ReLU')\n",
    "        previous_matrix_shape=current_weights.shape\n",
    "\n",
    "        if print_layer_shape==True:\n",
    "            print('Output shape of ',(nr_units_idx+1),'.hidden layer: ',current_nonlinear_representation.shape,'\\n')\n",
    "    \n",
    "    #Compute the predictions:\n",
    "    output_layer_weights=np.random.normal(size=current_nonlinear_representation.shape[1]).reshape(-1,1)\n",
    "    bias_output_layer=np.random.normal(size=1).reshape(-1,1)\n",
    "    \n",
    "    predictions=np.dot(current_nonlinear_representation,output_layer_weights)+bias_output_layer\n",
    "    if print_layer_shape==True:\n",
    "        print('Output shape of prediction layer: ',predictions.shape,'\\n')\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs shape:  (20, 5) \n",
      "\n",
      "Output shape of  1 .hidden layer:  (20, 100) \n",
      "\n",
      "Output shape of  2 .hidden layer:  (20, 50) \n",
      "\n",
      "Output shape of  3 .hidden layer:  (20, 10) \n",
      "\n",
      "Output shape of prediction layer:  (20, 1) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions_deep_net=deep_neural_net(hidden_units=[100,50,10],\n",
    "                                     inputs=data_batch,\n",
    "                                     print_layer_shape=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Implementation of deep feed-forward regression neural networks using sklearn.** <br>(Excercise 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, first, we will import a house pricing dataset from tensorflow, i.e., boston housing dataset. Afterward, we will examine some basic statistical characteristics of the features. Once the data is rescaled, we will train an MLPRegressor from sklearn to forecast the median price values of boston houses (our target variable). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train data: (404, 13) \n",
      "Shape of test data: (102, 13)\n"
     ]
    }
   ],
   "source": [
    "# this will import and split the data 80-20% by default:\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.boston_housing.load_data()\n",
    "\n",
    "print('Shape of train data:',x_train.shape,\n",
    "'\\nShape of test data:', x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The descriptions of the features are the following (more details about this dataset can be found in [http://lib.stat.cmu.edu/datasets/boston](http://lib.stat.cmu.edu/datasets/boston))<br>\n",
    "| # | Variable | Description |\n",
    "|---|---|---|\n",
    "| 1 | CRIM | per capita crime rate by town |\n",
    "| 2 | ZN | proportion of residential land zoned for lots over 25,000 sq.ft. |\n",
    "| 3 | INDUS | proportion of non-retail business acres per town |\n",
    "| 4 | CHAS | Charles River dummy variable (= 1 if tract bounds river; 0 otherwise) |\n",
    "| 5 | NOX | nitric oxides concentration (parts per 10 million) |\n",
    "| 6 | RM | average number of rooms per dwelling |\n",
    "| 7 | AGE | proportion of owner-occupied units built prior to 1940 |\n",
    "| 8 | DIS | weighted distances to five Boston employment centres |\n",
    "| 9 | RAD | index of accessibility to radial highways |\n",
    "| 10 | TAX | full-value property-tax rate per $10,000 |\n",
    "| 11 | PTRATIO | pupil-teacher ratio by town |\n",
    "| 12 | B | 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town |\n",
    "| 13 | LSTAT | % lower status of the population |\n",
    "\n",
    "So, we have 404 samples to train and 102 to test, each with 13 numerical features. The target is the median values of homes. Let's have a quick look at the predictor features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>404.000000</td>\n",
       "      <td>404.000000</td>\n",
       "      <td>404.000000</td>\n",
       "      <td>404.000000</td>\n",
       "      <td>404.000000</td>\n",
       "      <td>404.000000</td>\n",
       "      <td>404.000000</td>\n",
       "      <td>404.000000</td>\n",
       "      <td>404.000000</td>\n",
       "      <td>404.000000</td>\n",
       "      <td>404.000000</td>\n",
       "      <td>404.000000</td>\n",
       "      <td>404.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.745111</td>\n",
       "      <td>11.480198</td>\n",
       "      <td>11.104431</td>\n",
       "      <td>0.061881</td>\n",
       "      <td>0.557356</td>\n",
       "      <td>6.267082</td>\n",
       "      <td>69.010644</td>\n",
       "      <td>3.740271</td>\n",
       "      <td>9.440594</td>\n",
       "      <td>405.898515</td>\n",
       "      <td>18.475990</td>\n",
       "      <td>354.783168</td>\n",
       "      <td>12.740817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>9.240734</td>\n",
       "      <td>23.767711</td>\n",
       "      <td>6.811308</td>\n",
       "      <td>0.241238</td>\n",
       "      <td>0.117293</td>\n",
       "      <td>0.709788</td>\n",
       "      <td>27.940665</td>\n",
       "      <td>2.030215</td>\n",
       "      <td>8.698360</td>\n",
       "      <td>166.374543</td>\n",
       "      <td>2.200382</td>\n",
       "      <td>94.111148</td>\n",
       "      <td>7.254545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.006320</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.460000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.385000</td>\n",
       "      <td>3.561000</td>\n",
       "      <td>2.900000</td>\n",
       "      <td>1.129600</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>188.000000</td>\n",
       "      <td>12.600000</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>1.730000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.081437</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.130000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.453000</td>\n",
       "      <td>5.874750</td>\n",
       "      <td>45.475000</td>\n",
       "      <td>2.077100</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>279.000000</td>\n",
       "      <td>17.225000</td>\n",
       "      <td>374.672500</td>\n",
       "      <td>6.890000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.268880</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.690000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.538000</td>\n",
       "      <td>6.198500</td>\n",
       "      <td>78.500000</td>\n",
       "      <td>3.142300</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>330.000000</td>\n",
       "      <td>19.100000</td>\n",
       "      <td>391.250000</td>\n",
       "      <td>11.395000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.674808</td>\n",
       "      <td>12.500000</td>\n",
       "      <td>18.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.631000</td>\n",
       "      <td>6.609000</td>\n",
       "      <td>94.100000</td>\n",
       "      <td>5.118000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>666.000000</td>\n",
       "      <td>20.200000</td>\n",
       "      <td>396.157500</td>\n",
       "      <td>17.092500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>88.976200</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>27.740000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.871000</td>\n",
       "      <td>8.725000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>10.710300</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>711.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>396.900000</td>\n",
       "      <td>37.970000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               0           1           2           3           4           5   \\\n",
       "count  404.000000  404.000000  404.000000  404.000000  404.000000  404.000000   \n",
       "mean     3.745111   11.480198   11.104431    0.061881    0.557356    6.267082   \n",
       "std      9.240734   23.767711    6.811308    0.241238    0.117293    0.709788   \n",
       "min      0.006320    0.000000    0.460000    0.000000    0.385000    3.561000   \n",
       "25%      0.081437    0.000000    5.130000    0.000000    0.453000    5.874750   \n",
       "50%      0.268880    0.000000    9.690000    0.000000    0.538000    6.198500   \n",
       "75%      3.674808   12.500000   18.100000    0.000000    0.631000    6.609000   \n",
       "max     88.976200  100.000000   27.740000    1.000000    0.871000    8.725000   \n",
       "\n",
       "               6           7           8           9           10          11  \\\n",
       "count  404.000000  404.000000  404.000000  404.000000  404.000000  404.000000   \n",
       "mean    69.010644    3.740271    9.440594  405.898515   18.475990  354.783168   \n",
       "std     27.940665    2.030215    8.698360  166.374543    2.200382   94.111148   \n",
       "min      2.900000    1.129600    1.000000  188.000000   12.600000    0.320000   \n",
       "25%     45.475000    2.077100    4.000000  279.000000   17.225000  374.672500   \n",
       "50%     78.500000    3.142300    5.000000  330.000000   19.100000  391.250000   \n",
       "75%     94.100000    5.118000   24.000000  666.000000   20.200000  396.157500   \n",
       "max    100.000000   10.710300   24.000000  711.000000   22.000000  396.900000   \n",
       "\n",
       "               12  \n",
       "count  404.000000  \n",
       "mean    12.740817  \n",
       "std      7.254545  \n",
       "min      1.730000  \n",
       "25%      6.890000  \n",
       "50%     11.395000  \n",
       "75%     17.092500  \n",
       "max     37.970000  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(x_train).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The desriptive statistics show that the is a big variation in the mean and the standard deviation of the predictor variables. This indicates that the input features have different scales. For this reason, we will standardize our test dataset based on the statistics computed on the rescaled train dataset. With data rescaling, we ensure that no predictor dominates the input feature space due to varying variable scale. This, in turn, improves the stability of the training process, and facilitates faster convergence. The StandardScaler can be imported from sklearn.preprocessing: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html. Take a look at the code example provided under the link, and standardize the boston housing dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data rescaling:\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler=StandardScaler()\n",
    "scaler.fit(x_train)\n",
    "x_train_rescaled=scaler.transform(x_train)\n",
    "x_test_rescaled=scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we plot the distribution of the target variable on the train set, and examine whether it is also necessary to standardize the target in addition to the predictor variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plotting the distribution of target train values with a histogram from matplotlib.pyplot (imported as plt):\n",
    "plt.hist(y_train)\n",
    "plt.title('Distribution of Median Boston Housing Prices.')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the range of possible train target values is [0,50] in thousands we don't have to rescale the target variable. If the range of possible values had been [0,5.000.000], we would have rescaled the target, as then it would have had much higher variance. In such case, the scale of the target would cause huge errors during the training process of the neural network. This would in turn lead to very big gradients, which could destabilize the training process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we are completely done with the data preprocessing, we move on to training MLPRegressor() from sklearn.neural_network: https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html. After examining the possible hyperparameters, which can be specified in MLPRegressor(), we will train two neural models. The latter should use the Adam optimizer with the learning rate=0.001. Additionally, the first neural network should have a single Tanh-based hidden layer with 10 units, while the second network should apply ReLU in two hidden layers with the number of units [10,5]. Once the training process is complete, we will compare the performance of the two models on the test set using MSE. We defined the latter in a function in the first part of the notebook (the demo showing the forward passes). If you re-run the models several times, do you observe differences in the results? If yes, then why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results on Test Set: \n",
      "- MSE of Tanh-Neural Net:  391.88\n",
      "- MSE of ReLU-Neural Net:  39.957\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "tanh_MLP=MLPRegressor(hidden_layer_sizes=[10],\n",
    "                             activation='tanh', solver='adam',learning_rate_init=0.001)\n",
    "relu_MLP=MLPRegressor(hidden_layer_sizes=[10,5],\n",
    "                             activation='relu', solver='adam',learning_rate_init=0.001)\n",
    "\n",
    "tanh_MLP.fit(x_train_rescaled,y_train)\n",
    "relu_MLP.fit(x_train_rescaled,y_train)\n",
    "\n",
    "print('Results on Test Set: ')\n",
    "print('- MSE of Tanh-Neural Net: ',round(mse(true_target=y_test,predictions=tanh_MLP.predict(x_test_rescaled)),3))\n",
    "print('- MSE of ReLU-Neural Net: ',round(mse(true_target=y_test,predictions=relu_MLP.predict(x_test_rescaled)),3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In most of the runs, the ReLU model outperforms the Tanh network. This is no surprise, as the ReLU-network is slightly deeper than the Tanh-network, i.e., the ReLU-network has higher capacity than the Tanh-network. Also, by zeroing out negative inputs, ReLU achieves sparsity, which not only improves convergence, but also the generalization power of neural networks. Additionally, as we mentioned in the demo part, ReLU has the advantage over Tanh that ReLU does not suffer from vanishing gradients. The reason for the variation in the results from multiple runs is related to different initializations of the neural networks at each run. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. Hyperparameter Tuning using Optuna.** <br>(Excercise 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optuna represents an automatic hyperparameter optimization software framework, which supports multiple samplers, including the Tree-Parzen Estimator (TPE). This name implies a tree-structured search space, i.e., a search space that includes some conditional parameters, and use Parzen estimators such as kernel density estimators (KDEs). In a nutshell, TPE models the probability distribution of good and bad hyperparameter configurations, and then uses that to guide the search, i.e., to achieve a balance between exploration (trying new areas) and exploitation (focusing on promising regions).<br>\n",
    "\n",
    "Before we move on with the encoding of the search space using optuna, take a look at some code examples under the link: https://optuna.org/#code_examples. The hyperparameter tuning with optuna consists mainly of 5 aspects:\n",
    "- the definition of an objective function, which encodes the potentially conditional search space.\n",
    "- the creation of a study with optuna using a specific sampler, e.g., TPE. The study object should also  <br> \n",
    "  receive the direction for the optimization of the chosen metric, i.e., maximize accuracy or minimize an error metric.\n",
    "- the optimization of the hyperparameters for a pre-defined number of trials based on a score computed on the validation set. \n",
    "- extraction of the best hyperparameter configuration, and application of the best performing model on the test set.\n",
    "\n",
    "Next, we will reserve the last 104 samples from our train dataset for validation purposes, and we will define the following search space in Optuna's objective function:\n",
    "- For a maximum of 3 hidden layers sample values in [10,200] with stepsize=10. For deeper hidden layers allow optuna to sample also 0 units, indicating the network is not grown deeper.\n",
    "- Optimizer: ['adam','sgd']\n",
    "- Learning_Rate: [0.0001,0.001] with stepsize=0.0001\n",
    "- Activation=['identity', 'logistic', 'tanh', 'relu']\n",
    "\n",
    "Additionally, the alpha parameter, i.e., the L2 regularization coefficient, is set to the rather high value of 0.01 during the tuning process to avoid overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "#We reserve 104 samples from our training dataset for validation purposes. We should rescale the validation subset: \n",
    "x_train_optuna,x_val_optuna=x_train[:300,:],x_train[300:,:]\n",
    "\n",
    "optuna_scaler=StandardScaler()\n",
    "x_train_optuna_rescaled=optuna_scaler.fit_transform(x_train_optuna)\n",
    "x_val_optuna_rescaled=optuna_scaler.transform(x_val_optuna)\n",
    "\n",
    "y_train_optuna,y_val_optuna=y_train[:300],y_train[300:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definition of the objective as python function: \n",
    "# (Alternatively create a class for the objective of optuna: https://optuna.readthedocs.io/en/stable/faq.html)\n",
    "\n",
    "def objective(trial: optuna.Trial) -> np.float64:\n",
    "    '''\n",
    "    Perform hyperparameter tuning using optuna.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    trial:optuna.Trial\n",
    "        An optuna run with a specific hyperparameter configuration sampled in this function.\n",
    "\n",
    "    Returns: \n",
    "    ---------\n",
    "    mse_val: np.float64\n",
    "        The mse error score achieved on the validation set.\n",
    "    '''\n",
    "\n",
    "    #Sample number of neurons for a pre-defined maximum number of hidden layers: \n",
    "    # here you need to consider conditional hyperparameters:\n",
    "    max_hidden_layers=3\n",
    "    hidden_units_list=[]\n",
    "    keep_sampling_hidden_units=True\n",
    "    nr_hidden_layer=0\n",
    "    while keep_sampling_hidden_units:\n",
    "        if nr_hidden_layer==0:\n",
    "            current_hidden_units=trial.suggest_int(\"Units_hidden_layer_\"+str(nr_hidden_layer), low=10, high=200,step=10)\n",
    "        else:\n",
    "            current_hidden_units=trial.suggest_int(\"Units_hidden_layer_\"+str(nr_hidden_layer), low=0, high=200,step=10)\n",
    "        \n",
    "        if current_hidden_units==0 or nr_hidden_layer==max_hidden_layers-1:\n",
    "            keep_sampling_hidden_units=False\n",
    "        \n",
    "        if current_hidden_units!=0:\n",
    "            hidden_units_list.append(current_hidden_units)\n",
    "            \n",
    "        nr_hidden_layer=nr_hidden_layer+1\n",
    "    \n",
    "    #Non-conditional hyperparameters:\n",
    "    current_optimizer=trial.suggest_categorical(\"Optimizer\",['adam','sgd'])\n",
    "    current_learning_rate=trial.suggest_float(\"Learning_Rate\", low=0.0001, high=0.001,step=0.0001)\n",
    "    current_activation=trial.suggest_categorical(\"Activation\",['identity', 'logistic', 'tanh', 'relu'])\n",
    "\n",
    "    current_MLP_regressor=MLPRegressor(hidden_layer_sizes=hidden_units_list,\n",
    "                             activation=current_activation, solver=current_optimizer,learning_rate_init=current_learning_rate,\n",
    "                             alpha=0.01)\n",
    "    #Fit on the training set and evaluate the performance on validation set:\n",
    "    current_MLP_regressor.fit(x_train_optuna_rescaled,y_train_optuna)\n",
    "    val_predictions=current_MLP_regressor.predict(x_val_optuna_rescaled)\n",
    "    mse_val=mse(true_target=y_val_optuna,predictions=val_predictions)\n",
    "    return mse_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-17 10:40:10,699] A new study created in memory with name: no-name-bd422f04-0d70-4e65-8fcb-f12b3c5c6bc2\n",
      "C:\\Users\\Georg Velev\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "[I 2025-04-17 10:40:11,594] Trial 0 finished with value: 14.042603672626 and parameters: {'Units_hidden_layer_0': 60, 'Units_hidden_layer_1': 170, 'Units_hidden_layer_2': 0, 'Optimizer': 'adam', 'Learning_Rate': 0.0009000000000000001, 'Activation': 'relu'}. Best is trial 0 with value: 14.042603672626.\n",
      "[I 2025-04-17 10:40:12,782] Trial 1 finished with value: 28.65569368540267 and parameters: {'Units_hidden_layer_0': 130, 'Units_hidden_layer_1': 200, 'Units_hidden_layer_2': 90, 'Optimizer': 'adam', 'Learning_Rate': 0.00030000000000000003, 'Activation': 'identity'}. Best is trial 0 with value: 14.042603672626.\n",
      "C:\\Users\\Georg Velev\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "[I 2025-04-17 10:40:14,168] Trial 2 finished with value: 10.480589259126855 and parameters: {'Units_hidden_layer_0': 50, 'Units_hidden_layer_1': 40, 'Units_hidden_layer_2': 100, 'Optimizer': 'sgd', 'Learning_Rate': 0.00030000000000000003, 'Activation': 'relu'}. Best is trial 2 with value: 10.480589259126855.\n",
      "[I 2025-04-17 10:40:14,334] Trial 3 finished with value: 29.138061093942802 and parameters: {'Units_hidden_layer_0': 150, 'Units_hidden_layer_1': 120, 'Units_hidden_layer_2': 30, 'Optimizer': 'sgd', 'Learning_Rate': 0.001, 'Activation': 'identity'}. Best is trial 2 with value: 10.480589259126855.\n",
      "C:\\Users\\Georg Velev\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "[I 2025-04-17 10:40:17,032] Trial 4 finished with value: 19.35035020992319 and parameters: {'Units_hidden_layer_0': 190, 'Units_hidden_layer_1': 110, 'Units_hidden_layer_2': 200, 'Optimizer': 'adam', 'Learning_Rate': 0.0009000000000000001, 'Activation': 'tanh'}. Best is trial 2 with value: 10.480589259126855.\n",
      "C:\\Users\\Georg Velev\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "[I 2025-04-17 10:40:17,935] Trial 5 finished with value: 30.835379926001377 and parameters: {'Units_hidden_layer_0': 110, 'Units_hidden_layer_1': 0, 'Optimizer': 'sgd', 'Learning_Rate': 0.0008, 'Activation': 'logistic'}. Best is trial 2 with value: 10.480589259126855.\n",
      "[I 2025-04-17 10:40:18,122] Trial 6 finished with value: 30.449154828739953 and parameters: {'Units_hidden_layer_0': 110, 'Units_hidden_layer_1': 150, 'Units_hidden_layer_2': 20, 'Optimizer': 'sgd', 'Learning_Rate': 0.0007000000000000001, 'Activation': 'identity'}. Best is trial 2 with value: 10.480589259126855.\n",
      "C:\\Users\\Georg Velev\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "[I 2025-04-17 10:40:21,654] Trial 7 finished with value: 309.4423691546528 and parameters: {'Units_hidden_layer_0': 140, 'Units_hidden_layer_1': 170, 'Units_hidden_layer_2': 130, 'Optimizer': 'adam', 'Learning_Rate': 0.0002, 'Activation': 'logistic'}. Best is trial 2 with value: 10.480589259126855.\n",
      "C:\\Users\\Georg Velev\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "[I 2025-04-17 10:40:22,951] Trial 8 finished with value: 15.77884431955532 and parameters: {'Units_hidden_layer_0': 30, 'Units_hidden_layer_1': 170, 'Units_hidden_layer_2': 180, 'Optimizer': 'sgd', 'Learning_Rate': 0.0001, 'Activation': 'relu'}. Best is trial 2 with value: 10.480589259126855.\n",
      "C:\\Users\\Georg Velev\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "[I 2025-04-17 10:40:23,548] Trial 9 finished with value: 14.537163146775715 and parameters: {'Units_hidden_layer_0': 20, 'Units_hidden_layer_1': 20, 'Units_hidden_layer_2': 130, 'Optimizer': 'sgd', 'Learning_Rate': 0.0007000000000000001, 'Activation': 'tanh'}. Best is trial 2 with value: 10.480589259126855.\n",
      "C:\\Users\\Georg Velev\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "[I 2025-04-17 10:40:24,402] Trial 10 finished with value: 10.247917262347942 and parameters: {'Units_hidden_layer_0': 70, 'Units_hidden_layer_1': 50, 'Units_hidden_layer_2': 70, 'Optimizer': 'sgd', 'Learning_Rate': 0.0004, 'Activation': 'relu'}. Best is trial 10 with value: 10.247917262347942.\n",
      "C:\\Users\\Georg Velev\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "[I 2025-04-17 10:40:25,407] Trial 11 finished with value: 11.56654686401335 and parameters: {'Units_hidden_layer_0': 70, 'Units_hidden_layer_1': 50, 'Units_hidden_layer_2': 70, 'Optimizer': 'sgd', 'Learning_Rate': 0.0004, 'Activation': 'relu'}. Best is trial 10 with value: 10.247917262347942.\n",
      "C:\\Users\\Georg Velev\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "[I 2025-04-17 10:40:26,604] Trial 12 finished with value: 9.382631990388656 and parameters: {'Units_hidden_layer_0': 70, 'Units_hidden_layer_1': 60, 'Units_hidden_layer_2': 60, 'Optimizer': 'sgd', 'Learning_Rate': 0.0005, 'Activation': 'relu'}. Best is trial 12 with value: 9.382631990388656.\n",
      "C:\\Users\\Georg Velev\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "[I 2025-04-17 10:40:27,709] Trial 13 finished with value: 10.185667402071816 and parameters: {'Units_hidden_layer_0': 90, 'Units_hidden_layer_1': 80, 'Units_hidden_layer_2': 60, 'Optimizer': 'sgd', 'Learning_Rate': 0.0005, 'Activation': 'relu'}. Best is trial 12 with value: 9.382631990388656.\n",
      "C:\\Users\\Georg Velev\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "[I 2025-04-17 10:40:28,725] Trial 14 finished with value: 10.264156444077987 and parameters: {'Units_hidden_layer_0': 90, 'Units_hidden_layer_1': 80, 'Units_hidden_layer_2': 50, 'Optimizer': 'sgd', 'Learning_Rate': 0.0005, 'Activation': 'relu'}. Best is trial 12 with value: 9.382631990388656.\n",
      "C:\\Users\\Georg Velev\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "[I 2025-04-17 10:40:30,048] Trial 15 finished with value: 9.850354375249253 and parameters: {'Units_hidden_layer_0': 90, 'Units_hidden_layer_1': 80, 'Units_hidden_layer_2': 130, 'Optimizer': 'sgd', 'Learning_Rate': 0.0006000000000000001, 'Activation': 'relu'}. Best is trial 12 with value: 9.382631990388656.\n",
      "C:\\Users\\Georg Velev\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "[I 2025-04-17 10:40:31,184] Trial 16 finished with value: 10.032245642604673 and parameters: {'Units_hidden_layer_0': 40, 'Units_hidden_layer_1': 80, 'Units_hidden_layer_2': 140, 'Optimizer': 'sgd', 'Learning_Rate': 0.0006000000000000001, 'Activation': 'relu'}. Best is trial 12 with value: 9.382631990388656.\n",
      "C:\\Users\\Georg Velev\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "[I 2025-04-17 10:40:34,947] Trial 17 finished with value: 67.91439828732426 and parameters: {'Units_hidden_layer_0': 180, 'Units_hidden_layer_1': 120, 'Units_hidden_layer_2': 150, 'Optimizer': 'sgd', 'Learning_Rate': 0.0006000000000000001, 'Activation': 'logistic'}. Best is trial 12 with value: 9.382631990388656.\n",
      "C:\\Users\\Georg Velev\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "[I 2025-04-17 10:40:35,693] Trial 18 finished with value: 60.74921131267509 and parameters: {'Units_hidden_layer_0': 10, 'Units_hidden_layer_1': 70, 'Units_hidden_layer_2': 110, 'Optimizer': 'adam', 'Learning_Rate': 0.0007000000000000001, 'Activation': 'tanh'}. Best is trial 12 with value: 9.382631990388656.\n",
      "C:\\Users\\Georg Velev\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "[I 2025-04-17 10:40:36,485] Trial 19 finished with value: 10.374937598248748 and parameters: {'Units_hidden_layer_0': 80, 'Units_hidden_layer_1': 20, 'Units_hidden_layer_2': 180, 'Optimizer': 'sgd', 'Learning_Rate': 0.0004, 'Activation': 'relu'}. Best is trial 12 with value: 9.382631990388656.\n",
      "C:\\Users\\Georg Velev\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "[I 2025-04-17 10:40:37,978] Trial 20 finished with value: 10.457909248642714 and parameters: {'Units_hidden_layer_0': 120, 'Units_hidden_layer_1': 100, 'Units_hidden_layer_2': 90, 'Optimizer': 'sgd', 'Learning_Rate': 0.0006000000000000001, 'Activation': 'relu'}. Best is trial 12 with value: 9.382631990388656.\n",
      "C:\\Users\\Georg Velev\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "[I 2025-04-17 10:40:39,009] Trial 21 finished with value: 10.901721467840604 and parameters: {'Units_hidden_layer_0': 40, 'Units_hidden_layer_1': 70, 'Units_hidden_layer_2': 150, 'Optimizer': 'sgd', 'Learning_Rate': 0.0006000000000000001, 'Activation': 'relu'}. Best is trial 12 with value: 9.382631990388656.\n",
      "C:\\Users\\Georg Velev\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "[I 2025-04-17 10:40:40,389] Trial 22 finished with value: 10.410967904054154 and parameters: {'Units_hidden_layer_0': 40, 'Units_hidden_layer_1': 90, 'Units_hidden_layer_2': 130, 'Optimizer': 'sgd', 'Learning_Rate': 0.0005, 'Activation': 'relu'}. Best is trial 12 with value: 9.382631990388656.\n",
      "C:\\Users\\Georg Velev\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "[I 2025-04-17 10:40:41,889] Trial 23 finished with value: 9.364677803831832 and parameters: {'Units_hidden_layer_0': 90, 'Units_hidden_layer_1': 60, 'Units_hidden_layer_2': 160, 'Optimizer': 'sgd', 'Learning_Rate': 0.0008, 'Activation': 'relu'}. Best is trial 23 with value: 9.364677803831832.\n",
      "C:\\Users\\Georg Velev\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "[I 2025-04-17 10:40:43,295] Trial 24 finished with value: 10.781363817548655 and parameters: {'Units_hidden_layer_0': 90, 'Units_hidden_layer_1': 40, 'Units_hidden_layer_2': 160, 'Optimizer': 'sgd', 'Learning_Rate': 0.0008, 'Activation': 'relu'}. Best is trial 23 with value: 9.364677803831832.\n",
      "C:\\Users\\Georg Velev\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "[I 2025-04-17 10:40:45,045] Trial 25 finished with value: 11.308627481024494 and parameters: {'Units_hidden_layer_0': 160, 'Units_hidden_layer_1': 60, 'Units_hidden_layer_2': 170, 'Optimizer': 'sgd', 'Learning_Rate': 0.0008, 'Activation': 'relu'}. Best is trial 23 with value: 9.364677803831832.\n",
      "C:\\Users\\Georg Velev\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "[I 2025-04-17 10:40:46,062] Trial 26 finished with value: 37.81592536506069 and parameters: {'Units_hidden_layer_0': 100, 'Units_hidden_layer_1': 30, 'Units_hidden_layer_2': 110, 'Optimizer': 'adam', 'Learning_Rate': 0.001, 'Activation': 'tanh'}. Best is trial 23 with value: 9.364677803831832.\n",
      "C:\\Users\\Georg Velev\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "[I 2025-04-17 10:40:46,524] Trial 27 finished with value: 30.127497488311835 and parameters: {'Units_hidden_layer_0': 60, 'Units_hidden_layer_1': 0, 'Optimizer': 'sgd', 'Learning_Rate': 0.0007000000000000001, 'Activation': 'logistic'}. Best is trial 23 with value: 9.364677803831832.\n",
      "[I 2025-04-17 10:40:46,741] Trial 28 finished with value: 28.56671088373889 and parameters: {'Units_hidden_layer_0': 120, 'Units_hidden_layer_1': 130, 'Units_hidden_layer_2': 190, 'Optimizer': 'sgd', 'Learning_Rate': 0.0009000000000000001, 'Activation': 'identity'}. Best is trial 23 with value: 9.364677803831832.\n",
      "C:\\Users\\Georg Velev\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "[I 2025-04-17 10:40:47,462] Trial 29 finished with value: 13.229033627960234 and parameters: {'Units_hidden_layer_0': 60, 'Units_hidden_layer_1': 60, 'Units_hidden_layer_2': 40, 'Optimizer': 'adam', 'Learning_Rate': 0.0008, 'Activation': 'relu'}. Best is trial 23 with value: 9.364677803831832.\n",
      "C:\\Users\\Georg Velev\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "[I 2025-04-17 10:40:48,705] Trial 30 finished with value: 10.596160764846868 and parameters: {'Units_hidden_layer_0': 80, 'Units_hidden_layer_1': 100, 'Units_hidden_layer_2': 120, 'Optimizer': 'sgd', 'Learning_Rate': 0.0005, 'Activation': 'relu'}. Best is trial 23 with value: 9.364677803831832.\n",
      "C:\\Users\\Georg Velev\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "[I 2025-04-17 10:40:49,944] Trial 31 finished with value: 10.011826081647968 and parameters: {'Units_hidden_layer_0': 50, 'Units_hidden_layer_1': 90, 'Units_hidden_layer_2': 150, 'Optimizer': 'sgd', 'Learning_Rate': 0.0007000000000000001, 'Activation': 'relu'}. Best is trial 23 with value: 9.364677803831832.\n",
      "C:\\Users\\Georg Velev\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "[I 2025-04-17 10:40:51,496] Trial 32 finished with value: 11.266257536891827 and parameters: {'Units_hidden_layer_0': 70, 'Units_hidden_layer_1': 90, 'Units_hidden_layer_2': 160, 'Optimizer': 'sgd', 'Learning_Rate': 0.0007000000000000001, 'Activation': 'relu'}. Best is trial 23 with value: 9.364677803831832.\n",
      "C:\\Users\\Georg Velev\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "[I 2025-04-17 10:40:52,524] Trial 33 finished with value: 9.362693965229928 and parameters: {'Units_hidden_layer_0': 50, 'Units_hidden_layer_1': 60, 'Units_hidden_layer_2': 150, 'Optimizer': 'sgd', 'Learning_Rate': 0.0006000000000000001, 'Activation': 'relu'}. Best is trial 33 with value: 9.362693965229928.\n",
      "[I 2025-04-17 10:40:52,844] Trial 34 finished with value: 29.498360386544384 and parameters: {'Units_hidden_layer_0': 100, 'Units_hidden_layer_1': 60, 'Units_hidden_layer_2': 90, 'Optimizer': 'sgd', 'Learning_Rate': 0.00030000000000000003, 'Activation': 'identity'}. Best is trial 33 with value: 9.362693965229928.\n",
      "C:\\Users\\Georg Velev\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "[I 2025-04-17 10:40:53,829] Trial 35 finished with value: 16.02441571482246 and parameters: {'Units_hidden_layer_0': 80, 'Units_hidden_layer_1': 40, 'Units_hidden_layer_2': 170, 'Optimizer': 'adam', 'Learning_Rate': 0.0004, 'Activation': 'relu'}. Best is trial 33 with value: 9.362693965229928.\n",
      "C:\\Users\\Georg Velev\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "[I 2025-04-17 10:40:54,341] Trial 36 finished with value: 11.23040260943454 and parameters: {'Units_hidden_layer_0': 50, 'Units_hidden_layer_1': 70, 'Units_hidden_layer_2': 0, 'Optimizer': 'sgd', 'Learning_Rate': 0.0006000000000000001, 'Activation': 'relu'}. Best is trial 33 with value: 9.362693965229928.\n",
      "[I 2025-04-17 10:40:54,534] Trial 37 finished with value: 30.316626201592122 and parameters: {'Units_hidden_layer_0': 120, 'Units_hidden_layer_1': 30, 'Units_hidden_layer_2': 110, 'Optimizer': 'sgd', 'Learning_Rate': 0.0009000000000000001, 'Activation': 'identity'}. Best is trial 33 with value: 9.362693965229928.\n",
      "C:\\Users\\Georg Velev\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "[I 2025-04-17 10:40:55,292] Trial 38 finished with value: 11.872095332272217 and parameters: {'Units_hidden_layer_0': 60, 'Units_hidden_layer_1': 50, 'Units_hidden_layer_2': 80, 'Optimizer': 'sgd', 'Learning_Rate': 0.00030000000000000003, 'Activation': 'relu'}. Best is trial 33 with value: 9.362693965229928.\n",
      "C:\\Users\\Georg Velev\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "[I 2025-04-17 10:40:58,121] Trial 39 finished with value: 98.42406972303705 and parameters: {'Units_hidden_layer_0': 140, 'Units_hidden_layer_1': 110, 'Units_hidden_layer_2': 140, 'Optimizer': 'adam', 'Learning_Rate': 0.001, 'Activation': 'logistic'}. Best is trial 33 with value: 9.362693965229928.\n",
      "C:\\Users\\Georg Velev\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "[I 2025-04-17 10:40:59,390] Trial 40 finished with value: 17.237670839883442 and parameters: {'Units_hidden_layer_0': 30, 'Units_hidden_layer_1': 130, 'Units_hidden_layer_2': 200, 'Optimizer': 'sgd', 'Learning_Rate': 0.0005, 'Activation': 'tanh'}. Best is trial 33 with value: 9.362693965229928.\n",
      "C:\\Users\\Georg Velev\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "[I 2025-04-17 10:41:00,379] Trial 41 finished with value: 10.084786283244341 and parameters: {'Units_hidden_layer_0': 50, 'Units_hidden_layer_1': 90, 'Units_hidden_layer_2': 150, 'Optimizer': 'sgd', 'Learning_Rate': 0.0007000000000000001, 'Activation': 'relu'}. Best is trial 33 with value: 9.362693965229928.\n",
      "C:\\Users\\Georg Velev\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "[I 2025-04-17 10:41:02,064] Trial 42 finished with value: 9.22827977973808 and parameters: {'Units_hidden_layer_0': 70, 'Units_hidden_layer_1': 200, 'Units_hidden_layer_2': 140, 'Optimizer': 'sgd', 'Learning_Rate': 0.0008, 'Activation': 'relu'}. Best is trial 42 with value: 9.22827977973808.\n",
      "C:\\Users\\Georg Velev\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "[I 2025-04-17 10:41:03,658] Trial 43 finished with value: 9.48366836368742 and parameters: {'Units_hidden_layer_0': 70, 'Units_hidden_layer_1': 190, 'Units_hidden_layer_2': 140, 'Optimizer': 'sgd', 'Learning_Rate': 0.0008, 'Activation': 'relu'}. Best is trial 42 with value: 9.22827977973808.\n",
      "C:\\Users\\Georg Velev\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "[I 2025-04-17 10:41:05,711] Trial 44 finished with value: 9.92582157540311 and parameters: {'Units_hidden_layer_0': 70, 'Units_hidden_layer_1': 200, 'Units_hidden_layer_2': 170, 'Optimizer': 'sgd', 'Learning_Rate': 0.0008, 'Activation': 'relu'}. Best is trial 42 with value: 9.22827977973808.\n",
      "C:\\Users\\Georg Velev\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "[I 2025-04-17 10:41:06,928] Trial 45 finished with value: 10.532367230176542 and parameters: {'Units_hidden_layer_0': 70, 'Units_hidden_layer_1': 190, 'Units_hidden_layer_2': 20, 'Optimizer': 'sgd', 'Learning_Rate': 0.0009000000000000001, 'Activation': 'relu'}. Best is trial 42 with value: 9.22827977973808.\n",
      "C:\\Users\\Georg Velev\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "[I 2025-04-17 10:41:08,515] Trial 46 finished with value: 9.900334684849629 and parameters: {'Units_hidden_layer_0': 80, 'Units_hidden_layer_1': 190, 'Units_hidden_layer_2': 120, 'Optimizer': 'sgd', 'Learning_Rate': 0.0008, 'Activation': 'relu'}. Best is trial 42 with value: 9.22827977973808.\n",
      "C:\\Users\\Georg Velev\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "[I 2025-04-17 10:41:09,999] Trial 47 finished with value: 10.510666986653208 and parameters: {'Units_hidden_layer_0': 30, 'Units_hidden_layer_1': 160, 'Units_hidden_layer_2': 140, 'Optimizer': 'sgd', 'Learning_Rate': 0.0009000000000000001, 'Activation': 'relu'}. Best is trial 42 with value: 9.22827977973808.\n",
      "[I 2025-04-17 10:41:10,459] Trial 48 finished with value: 29.223554860139902 and parameters: {'Units_hidden_layer_0': 110, 'Units_hidden_layer_1': 190, 'Units_hidden_layer_2': 190, 'Optimizer': 'sgd', 'Learning_Rate': 0.0008, 'Activation': 'identity'}. Best is trial 42 with value: 9.22827977973808.\n",
      "C:\\Users\\Georg Velev\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "[I 2025-04-17 10:41:14,067] Trial 49 finished with value: 363.2339109113502 and parameters: {'Units_hidden_layer_0': 100, 'Units_hidden_layer_1': 180, 'Units_hidden_layer_2': 160, 'Optimizer': 'adam', 'Learning_Rate': 0.0001, 'Activation': 'logistic'}. Best is trial 42 with value: 9.22827977973808.\n"
     ]
    }
   ],
   "source": [
    "#Hyperparameter optimization:\n",
    "study = optuna.create_study(sampler=optuna.samplers.TPESampler(),direction='minimize')\n",
    "study.optimize(objective, n_trials=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best configuration:\n",
      "Units_hidden_layer_0 :  70\n",
      "Units_hidden_layer_1 :  200\n",
      "Units_hidden_layer_2 :  140\n",
      "Optimizer :  sgd\n",
      "Learning_Rate :  0.0008\n",
      "Activation :  relu\n"
     ]
    }
   ],
   "source": [
    "#Extraction of best hyperparameters and application of the model trained with the best configuration on the test set:\n",
    "best_config=study.best_trial.params\n",
    "print('Best configuration:')\n",
    "for key in best_config.keys():\n",
    "    print(key,': ',(best_config[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPRegressor(hidden_layer_sizes=[70, 200, 140], learning_rate_init=0.0008,\n",
       "             solver=&#x27;sgd&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;MLPRegressor<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.neural_network.MLPRegressor.html\">?<span>Documentation for MLPRegressor</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>MLPRegressor(hidden_layer_sizes=[70, 200, 140], learning_rate_init=0.0008,\n",
       "             solver=&#x27;sgd&#x27;)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "MLPRegressor(hidden_layer_sizes=[70, 200, 140], learning_rate_init=0.0008,\n",
       "             solver='sgd')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_mlp_regressor=MLPRegressor(hidden_layer_sizes=[best_config['Units_hidden_layer_0'],best_config['Units_hidden_layer_1'], best_config['Units_hidden_layer_2']],\n",
    "                             activation=best_config['Activation'], solver=best_config['Optimizer'],learning_rate_init=best_config['Learning_Rate'])\n",
    "best_mlp_regressor.fit(x_train_rescaled,y_train)                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results on Test Set: \n",
      "- MSE of not tuned ReLU-Neural Net:  39.957\n",
      "- MSE of tuned Neural Net:  23.747\n"
     ]
    }
   ],
   "source": [
    "#Comparison of results with tuned vs not tuned neural network:\n",
    "print('Results on Test Set: ')\n",
    "print('- MSE of not tuned ReLU-Neural Net: ',round(mse(true_target=y_test,predictions=relu_MLP.predict(x_test_rescaled)),3))\n",
    "print('- MSE of tuned Neural Net: ',round(mse(true_target=y_test,predictions=best_mlp_regressor.predict(x_test_rescaled)),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e2c68f0abcf3705a60d2a1ddd11382db992c60d9e301bdf771b0d9399403a061"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
